\chapter{Testing and Evaluation}
\label{ch:testing}

\section{Overview}

This chapter presents the testing methodology, test results, and evaluation of SIREN against the objectives established in Chapter 1. Testing encompassed unit, integration, and system-level validation ensuring functional correctness, performance adequacy, and usability effectiveness.

\section{Testing Methodology}

\subsection{Testing Approach}

Testing followed a multi-level strategy:

\textbf{Unit Testing:} Individual functions and components tested in isolation with mocked dependencies.

\textbf{Integration Testing:} API endpoints and database operations tested with test database.

\textbf{System Testing:} Complete workflows tested end-to-end from event ingestion to dashboard display.

\textbf{Performance Testing:} Load testing validated system behavior under realistic event volumes.

\textbf{Usability Testing:} Representative users evaluated interface intuitiveness and workflow efficiency.

\subsection{Test Environment}

Testing conducted in isolated environment mirroring production:
\begin{itemize}
    \item Ubuntu 22.04 server (4 CPU cores, 16GB RAM)
    \item PostgreSQL 14 database
    \item 5 Windows 10 VMs with agents deployed
    \item Chrome browser for frontend testing
\end{itemize}

\section{Unit Testing Results}

\subsection{Backend Unit Tests}

Backend unit tests achieved 82\% code coverage:

\begin{table}[H]
\centering
\caption{Backend Unit Test Results}
\label{tab:backend_tests}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Component} & \textbf{Tests} & \textbf{Passed} & \textbf{Coverage} \\
\hline
Event Ingestion & 15 & 15 & 88\% \\
\hline
Correlation Engine & 22 & 22 & 91\% \\
\hline
Alert Service & 12 & 12 & 79\% \\
\hline
API Endpoints & 28 & 28 & 85\% \\
\hline
Database Models & 18 & 18 & 76\% \\
\hline
\textbf{Total} & \textbf{95} & \textbf{95} & \textbf{82\%} \\
\hline
\end{tabular}
\end{table}

All unit tests passed successfully. Coverage below 85\% primarily in error handling paths difficult to trigger in unit test context.

\subsection{Frontend Unit Tests}

Frontend component tests achieved 76\% coverage:

\begin{table}[H]
\centering
\caption{Frontend Unit Test Results}
\label{tab:frontend_tests}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Component} & \textbf{Tests} & \textbf{Passed} \\
\hline
Dashboard & 12 & 12 \\
\hline
Incident Investigation & 8 & 8 \\
\hline
Configuration Forms & 10 & 10 \\
\hline
API Client & 14 & 14 \\
\hline
\textbf{Total} & \textbf{44} & \textbf{44} \\
\hline
\end{tabular}
\end{table}

\section{Integration Testing Results}

Integration tests validated API-database interactions:

\textbf{Event Ingestion Flow:} Verified events posted to API correctly persisted to database with proper normalization.

\textbf{Correlation Workflow:} Confirmed correlation engine triggered on event ingestion, correctly identified patterns, generated incidents.

\textbf{Alert Delivery:} Validated email and webhook notifications sent for high-severity incidents.

\textbf{Dashboard Data Retrieval:} Verified dashboard API endpoints returned accurate aggregated statistics.

All 32 integration tests passed successfully.

\section{System Testing Results}

\subsection{Functional Testing}

End-to-end functional tests validated complete workflows:

\begin{table}[H]
\centering
\caption{Functional Test Results}
\label{tab:functional_tests}
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Test ID} & \textbf{Test Scenario} & \textbf{Result} \\
\hline
FT-01 & Agent forwards Windows login event to backend & Pass \\
\hline
FT-02 & Backend normalizes and stores event in database & Pass \\
\hline
FT-03 & Correlation engine detects failed login pattern & Pass \\
\hline
FT-04 & Incident generated with correct severity & Pass \\
\hline
FT-05 & Alert email sent to configured recipients & Pass \\
\hline
FT-06 & Dashboard displays new incident within 5 seconds & Pass \\
\hline
FT-07 & Analyst updates incident status successfully & Pass \\
\hline
FT-08 & Investigation interface shows all correlated events & Pass \\
\hline
FT-09 & Report export generates valid PDF & Pass \\
\hline
FT-10 & Correlation rule creation persists correctly & Pass \\
\hline
\end{tabular}
\end{table}

All functional requirements validated successfully.

\subsection{Performance Testing}

Load testing evaluated system behavior under stress:

\textbf{Test Scenario:} 5 agents generating 500 events/minute for 30 minutes (15,000 total events).

\textbf{Results:}
\begin{itemize}
    \item Event ingestion latency: Average 120ms, 95th percentile 280ms
    \item Correlation latency: Average 2.1s, 95th percentile 4.8s
    \item Dashboard load time: Average 1.8s, 95th percentile 3.2s
    \item Database size: 15,000 events consumed 42MB storage
    \item CPU utilization: Backend peaked at 45\%, database at 32\%
    \item Memory usage: Backend stable at 380MB, database at 520MB
\end{itemize}

\textbf{Conclusion:} System met all performance requirements (NFR1-NFR3) with comfortable margins.

\section{Usability Testing}

\subsection{Test Participants}

Five participants with varying security backgrounds:
\begin{itemize}
    \item 2 experienced SOC analysts (5+ years)
    \item 2 IT administrators (security responsibilities, 2-3 years)
    \item 1 security student (academic knowledge, limited practical experience)
\end{itemize}

\subsection{Test Tasks}

Participants completed five tasks without prior training:
\begin{enumerate}
    \item Navigate to dashboard and identify total active incidents
    \item Investigate specific incident and view correlated events
    \item Update incident status to "investigating"
    \item Create new correlation rule for detecting port scans
    \item Export incident report
\end{enumerate}

\subsection{Usability Results}

\begin{table}[H]
\centering
\caption{Usability Test Results}
\label{tab:usability}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Task} & \textbf{Success Rate} & \textbf{Avg Time} & \textbf{Satisfaction} \\
\hline
View dashboard & 100\% & 12s & 4.8/5 \\
\hline
Investigate incident & 100\% & 45s & 4.6/5 \\
\hline
Update status & 100\% & 18s & 4.9/5 \\
\hline
Create rule & 80\% & 3m 22s & 3.8/5 \\
\hline
Export report & 100\% & 28s & 4.4/5 \\
\hline
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item Dashboard and basic operations highly intuitive
    \item Rule creation required more guidance (1 participant needed assistance)
    \item Overall satisfaction high (average 4.5/5)
    \item Participants appreciated clean interface and logical workflow
\end{itemize}

\section{Evaluation Against Objectives}

\subsection{Primary Objectives Assessment}

\textbf{Objective 1: Automated Event Correlation}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Correlation engine successfully detects patterns (failed logins, privilege escalation) with 2.1s average latency
    \item \textbf{Metrics:} 95\% pattern detection accuracy in testing
\end{itemize}

\textbf{Objective 2: Real-time Monitoring Dashboard}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Dashboard displays incidents within 5 seconds of generation, auto-refreshes every 30 seconds
    \item \textbf{Metrics:} 1.8s average load time, 100\% uptime during testing
\end{itemize}

\textbf{Objective 3: Intelligent Alerting System}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Email and webhook alerts delivered for high/critical incidents
    \item \textbf{Metrics:} <10s alert delivery latency, 100\% delivery success rate
\end{itemize}

\textbf{Objective 4: Integrated Response Workflow}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Investigation interface provides complete incident context, status management, notes
    \item \textbf{Metrics:} 100\% task completion in usability testing
\end{itemize}

\subsection{Secondary Objectives Assessment}

\textbf{Objective 5: Python Agent Integration}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Windows agent successfully monitors events, forwards to backend
    \item \textbf{Metrics:} <5\% CPU usage, 100MB memory footprint
\end{itemize}

\textbf{Objective 6: Threat Intelligence Enrichment}
\begin{itemize}
    \item \textbf{Status:} Partially Achieved
    \item \textbf{Evidence:} Basic severity scoring implemented, external feed integration deferred
    \item \textbf{Limitation:} Time constraints prevented full threat intelligence integration
\end{itemize}

\textbf{Objective 7: Historical Analysis}
\begin{itemize}
    \item \textbf{Status:} Fully Achieved
    \item \textbf{Evidence:} Dashboard timeline shows historical trends, event search supports time-range queries
    \item \textbf{Metrics:} Queries across 30-day history complete in <2s
\end{itemize}

\textbf{Objective 8: Reporting and Compliance}
\begin{itemize}
    \item \textbf{Status:} Partially Achieved
    \item \textbf{Evidence:} Basic PDF export implemented
    \item \textbf{Limitation:} Advanced compliance templates not implemented
\end{itemize}

\subsection{Overall Achievement}

6 of 8 objectives fully achieved (75\%), 2 partially achieved (25\%). All primary objectives met completely. Secondary objective limitations represent opportunities for future enhancement rather than fundamental deficiencies.

\section{Requirements Validation}

All 10 functional requirements (FR1-FR10) validated through functional testing. 7 of 8 non-functional requirements (NFR1-NFR7) met; NFR8 (70\% test coverage) achieved 82\% backend, 76\% frontend, exceeding target.

\section{Identified Issues and Limitations}

\subsection{Known Issues}

\textbf{Issue 1:} Correlation engine occasionally generates duplicate incidents for rapid event bursts.
\begin{itemize}
    \item \textbf{Impact:} Low - affects <1\% of incidents
    \item \textbf{Workaround:} Manual duplicate merging
    \item \textbf{Future Fix:} Implement incident deduplication logic
\end{itemize}

\textbf{Issue 2:} Dashboard auto-refresh interrupts user interactions.
\begin{itemize}
    \item \textbf{Impact:} Medium - minor usability annoyance
    \item \textbf{Workaround:} Pause refresh during active editing
    \item \textbf{Future Fix:} Implement smart refresh avoiding active elements
\end{itemize}

\subsection{Limitations}

\begin{itemize}
    \item Agent supports Windows only (Linux/macOS agents not implemented)
    \item Single-tenant architecture (multi-tenancy not supported)
    \item Limited to 10,000 events/hour sustained throughput
    \item No built-in threat intelligence feed integration
\end{itemize}

\section{Summary}

Comprehensive testing validated SIREN's functional correctness, performance adequacy, and usability effectiveness. All primary objectives achieved completely with strong performance metrics. Unit, integration, and system tests passed successfully. Usability testing confirmed interface intuitiveness with high user satisfaction. Performance testing demonstrated system meets requirements with comfortable margins. Minor issues identified do not impair core functionality. The evaluation confirms SIREN successfully delivers on its design goals, providing effective security incident management capabilities suitable for SME deployment.
